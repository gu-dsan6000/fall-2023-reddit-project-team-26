{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8ae429-47c6-4761-982f-512698bbf719",
   "metadata": {},
   "source": [
    "# Project EDA Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6cc23-dcb0-4013-8318-7d0f80d47dbf",
   "metadata": {},
   "source": [
    "## Business Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c13ef86-d4b4-4451-a1e1-7253311062a6",
   "metadata": {},
   "source": [
    "### Setting up Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3833a1d-4c28-4a73-b435-96535375a4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.9.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.9.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.08.22 |       h06a4308_0         123 KB\n",
      "    certifi-2023.7.22          |  py310h06a4308_0         153 KB\n",
      "    openjdk-11.0.13            |       h87a67e3_0       341.0 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       341.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            pkgs/main/linux-64::openjdk-11.0.13-h87a67e3_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.7.2~ --> pkgs/main::ca-certificates-2023.08.22-h06a4308_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2023.7.22~ --> pkgs/main/linux-64::certifi-2023.7.22-py310h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2023.7.22    | 153 KB    |                                       |   0% \n",
      "ca-certificates-2023 | 123 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "certifi-2023.7.22    | 153 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "ca-certificates-2023 | 123 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | 3                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #8                                    |   5% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###4                                  |   9% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ####9                                 |  13% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ######3                               |  17% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #######7                              |  21% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #########1                            |  25% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##########5                           |  28% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###########8                          |  32% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #############2                        |  36% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##############6                       |  40% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ################                      |  43% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #################4                    |  47% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##################8                   |  51% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ####################1                 |  54% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #####################3                |  58% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ######################4               |  61% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #######################5              |  64% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ########################5             |  66% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #########################8            |  70% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##########################9           |  73% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ############################          |  76% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #############################2        |  79% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##############################7       |  83% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ################################      |  87% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #################################2    |  90% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##################################3   |  93% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###################################5  |  96% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ####################################8 | 100% \u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyspark==3.3.0\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5 (from pyspark==3.3.0)\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764004 sha256=29385091102db38f480e1b3bcf2bf4bd5669ff9198ec0cbf4505c1c905ff9cdf\n",
      "  Stored in directory: /root/.cache/pip/wheels/81/9c/6c/d5200fcf351ffa39cbe09911e99703283624cd037df58070d9\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.3.0\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b128071-13f5-49cf-8ee3-02755555d4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-78e343e2-3ab5-45b4-aa2e-ff350e9a9f84;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 655ms :: artifacts dl 41ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-78e343e2-3ab5-45b4-aa2e-ff350e9a9f84\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/29ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/29 16:30:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070acf71-a6dc-4b37-8c03-56e1d721cee0",
   "metadata": {},
   "source": [
    "### Reading in Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec903cab-0334-4d74-a3e4-82092ab06be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/29 16:31:58 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+---------+----------+------------+-----+--------+--------------------+------------+\n",
      "|             author|author_flair_css_class|   author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|  link_id| parent_id|retrieved_on|score|stickied|           subreddit|subreddit_id|\n",
      "+-------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+---------+----------+------------+-----+--------+--------------------+------------+\n",
      "|          y26404986|                  null|                null|Mmmmm ... is this...|               0| 1641098735|         null|  null|     0|hqwcail|t3_rtzn1c| t3_rtzn1c|  1645567209|    1|   false|          RedditSets|   t5_3psukr|\n",
      "|            iTwango|                  null|                null|\"Aren't you a lit...|               0| 1641098735|         null|  null|     0|hqwcaim|t3_rtwci2|t1_hqvbwwp|  1645567209|    2|   false|   NoStupidQuestions|    t5_2w844|\n",
      "|       SandiegoJack|                  null|                null|“Full”\\n\\nAlso if...|               0| 1641098735|         null|  null|     0|hqwcain|t3_rsu2gf|t1_hqp04wj|  1645567209|    1|   false|WarhammerCompetitive|    t5_3mrre|\n",
      "|        Happy-Kevin|                  null|                null|Could possibly be...|               0| 1641098735|         null|  null|     0|hqwcaio|t3_rop938|t1_hq3sq1e|  1645567209|    1|   false|          OculusLink|   t5_25gzcy|\n",
      "|         branden110|  oklahoma-sheet1-r...|:oklahoma: :wyomi...|YOURE GONNA DIE I...|               0| 1641098735|         null|  null|     0|hqwcaip|t3_rtx6ev|t1_hqwc6yz|  1645567209|    2|   false|                 CFB|    t5_2qm9d|\n",
      "|         imsexytoby|                  null|                null|My husband once n...|               0| 1641098735|         null|  null|     0|hqwcaiq|t3_rtfail|t1_hqst17e|  1645567209|    1|   false|       breastfeeding|    t5_2rdj4|\n",
      "|      tryantsupermo|                  null|        Kleptomaniac|Only in time Ochacko|               0| 1641098735|         null|  null|     0|hqwcair|t3_rtja2w| t3_rtja2w|  1645567209|    5|   false|  BokuNoMetaAcademia|    t5_9mbir|\n",
      "|        jstoltpojke|                  null|                null|Where do we get t...|               0| 1641098735|         null|  null|     0|hqwcais|t3_ru1hre| t3_ru1hre|  1645567209|    1|   false|           jerkbudss|   t5_3b1ay1|\n",
      "|          ManaMonoR|                  null|                null|ate some hot dogs...|               0| 1641098735|         null|  null|     0|hqwcait|t3_ru1km9|t1_hqwc6za|  1645567209|    2|   false|              delta8|   t5_2kpdau|\n",
      "|         cassity282|                  null|    Partassipant [4]|why would you ass...|               0| 1641098735|         null|  null|     0|hqwcaiu|t3_rtugjy|t1_hqw4xt7|  1645567209|   37|   false|       AmItheAsshole|    t5_2xhvq|\n",
      "|     BalanceMaestro|                  null|Moron, son of Mor...|Yes.  some people...|               0| 1641098735|         null|  null|     0|hqwcaiv|t3_rtq1hj| t3_rtq1hj|  1645567209|    2|   false|            exmormon|    t5_2r0gj|\n",
      "|      MrVictor01010|                  null|OU - Computer Eng...|       Did you pass?|               0| 1641098735|         null|  null|     0|hqwcaiw|t3_kwimro|t1_gj7ws35|  1645567209|    1|   false| EngineeringStudents|    t5_2sh0b|\n",
      "|            inminit|                  null|                null|I saw this.\\n\\n&a...|               0| 1641098735|         null|  null|     0|hqwcaix|t3_ru0q6u|t1_hqw9lpw|  1645567209|    5|   false|    AlgorandOfficial|    t5_wm2x2|\n",
      "|       xxxC0Y0T3xxx|                  null|                null|They were a bit s...|               0| 1641098735|         null|  null|     0|hqwcaiy|t3_rpzvm5|t1_hqwc3gs|  1645567209|    2|   false|     WorldCrossovers|   t5_2i51nq|\n",
      "| SharedZoneBeyMoist|                  null|                null|Dry Herb Vape, I'...|               0| 1641098735|         null|  null|     0|hqwcaiz|t3_rti5m6|t1_hqsvwbw|  1645567209|    0|   false|             uktrees|    t5_2si9d|\n",
      "|          [deleted]|                  null|                null|           [removed]|               0| 1641098735|         null|  null|     0|hqwcaj0|t3_rtx6ev| t3_rtx6ev|  1645567209|    7|   false|                 CFB|    t5_2qm9d|\n",
      "|     twinnedcalcite|                  null|                null|No a single place...|               0| 1641098735|         null|  null|     0|hqwcaj1|t3_rt95xd|t1_hquwpnt|  1645567209|    1|   false|          iceskating|    t5_2rguz|\n",
      "|             FE4R3D|                  null|                null|I was just prescr...|               0| 1641098735|         null|  null|     0|hqwcaj2|t3_rtptnx| t3_rtptnx|  1645567209|    1|   false|              Asthma|    t5_2s8vp|\n",
      "|Dazzling_Ostrich606|                  null|                null|I never realized ...|               0| 1641098735|         null|  null|     0|hqwcaj3|t3_rtx1lp|t1_hqw1zmm|  1645567209|    3|   false|       marvelstudios|    t5_2uii8|\n",
      "|  Every_Thought5834|                  null|                null|Been in your shoe...|               0| 1641098735|         null|  null|     0|hqwcaj4|t3_ru0mzq| t3_ru0mzq|  1645567209|    5|   false|            Marriage|    t5_2reak|\n",
      "+-------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+---------+----------+------------+-----+--------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 233 ms, sys: 37.6 ms, total: 271 ms\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "comments = spark.read.parquet(\n",
    "    \"s3a://bigdatateaching/reddit/parquet/comments/yyyy=*/mm=*/*comments*.parquet\",\n",
    "    header=True\n",
    ")\n",
    "comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a12314a-998f-46be-9de2-dc5a9b1b89c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+--------------------+-----------+-------------+--------------------+-------------+------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+--------------------+------------+--------------------+--------------------+\n",
      "|            author|author_flair_css_class|   author_flair_text|created_utc|distinguished|              domain|       edited|    id|is_self|locked|num_comments|over_18|quarantine|retrieved_on|score|            selftext|stickied|           subreddit|subreddit_id|               title|                 url|\n",
      "+------------------+----------------------+--------------------+-----------+-------------+--------------------+-------------+------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+--------------------+------------+--------------------+--------------------+\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|           i.redd.it|         null|rkrds9|  false| false|           0|  false|     false|  1656064284|    3|           [deleted]|   false|antitheistcheesecake|   t5_56ml5q|           straw man|https://i.redd.it...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|self.antitheistch...|         null|rkrhyh|   true| false|           1|  false|     false|  1656064280|    1|           [removed]|   false|antitheistcheesecake|   t5_56ml5q|   [deleted by user]|https://www.reddi...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|           i.redd.it|         null|rkrjf5|  false| false|          12|  false|     false|  1656064278|   55|                    |   false|antitheistcheesecake|   t5_56ml5q|The sexual revolu...|https://i.redd.it...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|self.antitheistch...|         null|rkrk3b|   true| false|           1|  false|     false|  1656064278|    2|           [removed]|   false|antitheistcheesecake|   t5_56ml5q|   [deleted by user]|https://www.reddi...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|           i.redd.it|         null|rkrlu7|  false| false|           6|  false|     false|  1656064276|   41|                    |   false|antitheistcheesecake|   t5_56ml5q|in reality is the...|https://i.redd.it...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|self.antitheistch...|         null|rkrmkl|   true| false|           1|  false|     false|  1656064276|    1|           [removed]|   false|antitheistcheesecake|   t5_56ml5q|   [deleted by user]|https://www.reddi...|\n",
      "|            qatlrn|                  null|                null| 1640996362|    moderator|  self.ProjectListen|1.640996737E9|rt0ohv|   true| false|          28|  false|     false|  1654200024|   14|As we bring in th...|   false|       ProjectListen|   t5_4ie55f|Introducing The P...|https://www.reddi...|\n",
      "|           NBA_MOD|                   NBA|  [NBA] Scott Foster| 1640995200|         null|            self.nba|1.641006311E9|rt6ogv|   true| false|          21|  false|     false|  1654199632|    9|##General Informa...|   false|                 nba|    t5_2qo4s|GAME THREAD: New ...|https://www.reddi...|\n",
      "|      FPPenSwapBot|                  null|                 Bot| 1640995200|         null|       self.Pen_Swap|         null|rt6ogw|   true| false|        1884|  false|     false|  1654199632|   22|Post your confirm...|   false|            Pen_Swap|    t5_2tyn2|January 2022 Conf...|https://www.reddi...|\n",
      "|    Donald_Filimon|                  null|        🎨 Artist 🎨| 1640995200|         null|          opensea.io|         null|rt6ogx|  false| false|           2|  false|     false|  1654199632|    1|                    |   false|     NFTsMarketplace|   t5_4147ke|CryptoCoinBank – ...|https://opensea.i...|\n",
      "|       YarnSwapBot|                  null|                 Bot| 1640995200|         null|       self.Yarnswap|         null|rt6ogy|   true| false|          16|  false|     false|  1654199632|    2|Post your confirm...|   false|            Yarnswap|    t5_2rx58|January 2022 Conf...|https://www.reddi...|\n",
      "|           NBA_MOD|                   NBA|  [NBA] Scott Foster| 1640995200|         null|            self.nba|1.641007038E9|rt6ogz|   true| false|          26|  false|     false|  1654199632|   11|##General Informa...|   false|                 nba|    t5_2qo4s|GAME THREAD: San ...|https://www.reddi...|\n",
      "|         [deleted]|                  null|                null| 1640995200|         null|    self.isthislegal|         null|rt6oh0|   true| false|           2|  false|     false|  1654199632|    3|           [removed]|   false|         isthislegal|    t5_2t5h4|   [deleted by user]|https://www.reddi...|\n",
      "|          OtterSou|                  null|                null| 1640995200|         null|        self.ISO8601|         null|rt6oh1|   true| false|          13|  false|     false|  1654199632|  404|Here's to another...|   false|             ISO8601|    t5_2tez4|Happy 2022-01-01T...|https://www.reddi...|\n",
      "|       nearbyareas|                  null|                null| 1640995200|         null|           self.COPD|         null|rt6oh2|   true| false|          13|  false|     false|  1654199632|    3|Also can PFT test...|   false|                COPD|    t5_2wk9n|Copd life expectancy|https://www.reddi...|\n",
      "|           Tokyono|                  null|                null| 1640995200|         null|           i.redd.it|         null|rt6oh5|  false| false|           0|  false|     false|  1654199632|   32|                    |   false|              museum|    t5_2rti0|Briton Riviere - ...|https://i.redd.it...|\n",
      "|Ok-Assignment-6458|                  null|                null| 1640995200|         null|         redgifs.com|         null|rt6oh6|  false| false|           0|   true|     false|  1654199632|    6|                    |   false|       HaveToHaveHer|    t5_3brmn|Blonde With A Nic...|https://www.redgi...|\n",
      "|        Ok-Fly1879|                  null|                null| 1640995200|         null|           i.redd.it|         null|rt6oh7|  false| false|           9|  false|     false|  1654199632|  125|                    |   false|       DannyGonzalez|    t5_mljgb|      Danny GONzalez|https://i.redd.it...|\n",
      "|    Shimmering-Sky|                      |:AMQ::STAR::MAL:h...| 1640995200|         null|          self.anime|         null|rt6oh8|   true| false|          44|  false|     false|  1654199632|   32|#***[Hajimari wa ...|   false|               anime|    t5_2qh22|REMINDER - Cross ...|https://www.reddi...|\n",
      "|          Abyss002|                  null|                null| 1640995200|         null|     self.allthemods|         null|rt6oha|   true| false|           0|  false|     false|  1654199632|    1|           [removed]|   false|          allthemods|    t5_32q4b|[Modded Server | ...|https://www.reddi...|\n",
      "+------------------+----------------------+--------------------+-----------+-------------+--------------------+-------------+------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 24.2 ms, sys: 6.87 ms, total: 31.1 ms\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submissions = spark.read.parquet(\n",
    "    \"s3a://bigdatateaching/reddit/parquet/submissions/yyyy=*/mm=*/*submissions*.parquet\",\n",
    "    header=True\n",
    ")\n",
    "submissions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41d17ed-cd58-4694-8fe0-d457b01e65af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the submissions dataframe is 313,972,841x21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>(3502 + 1) / 3503]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 3,076,907,105x17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\") \n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb76ec-baf0-43c1-b4f2-0ba185624d81",
   "metadata": {},
   "source": [
    "### Selecting Desired Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3095385-c902-4de1-bff3-d7e3a988a85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1407464b-0fb3-4205-ada6-8d321ccdf2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/process.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n",
    "    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n",
    "    parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    logger.info(f\"spark version = {spark.version}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n",
    "    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n",
    "    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # filter the dataframe to only keep the subreddits of interest\n",
    "    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n",
    "    submissions_filtered = submissions.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    comments_filtered = comments.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    \n",
    "    # save the filtered dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments\"\n",
    "    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n",
    "    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions\"\n",
    "    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n",
    "    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d313e0a3-94eb-462c-997f-50338e51281b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      ".................................................................................................................................................................................................................................*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Processing job sm-spark-project-2023-10-29-17-54-53-000: Failed. Reason: OutOfMemoryError: Please use an instance type with more memory, or ensure that your processing container does not use more memory than available.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:27\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/spark/processing.py:921\u001b[0m, in \u001b[0;36mPySparkProcessor.run\u001b[0;34m(self, submit_app, submit_py_files, submit_jars, submit_files, inputs, outputs, arguments, wait, logs, job_name, experiment_config, configuration, spark_event_logs_s3_uri, kms_key)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmit_app is required\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    911\u001b[0m extended_inputs, extended_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_processing_args(\n\u001b[1;32m    912\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    913\u001b[0m     outputs\u001b[38;5;241m=\u001b[39moutputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m     spark_event_logs_s3_uri\u001b[38;5;241m=\u001b[39mspark_event_logs_s3_uri,\n\u001b[1;32m    919\u001b[0m )\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubmit_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubmit_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_current_job_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/spark/processing.py:260\u001b[0m, in \u001b[0;36m_SparkProcessorBase.run\u001b[0;34m(self, submit_app, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_pipeline_variable(submit_app):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmit_app argument has to be a valid S3 URI or local file path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrather than a pipeline variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m     )\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubmit_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/processing.py:688\u001b[0m, in \u001b[0;36mScriptProcessor.run\u001b[0;34m(self, code, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_job)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/processing.py:1115\u001b[0m, in \u001b[0;36mProcessingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mlogs_for_processing_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_processing_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:4413\u001b[0m, in \u001b[0;36mSession.wait_for_processing_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   4399\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for an Amazon SageMaker Processing job to complete.\u001b[39;00m\n\u001b[1;32m   4400\u001b[0m \n\u001b[1;32m   4401\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4410\u001b[0m \u001b[38;5;124;03m    exceptions.UnexpectedStatusException: If the processing job fails.\u001b[39;00m\n\u001b[1;32m   4411\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4412\u001b[0m desc \u001b[38;5;241m=\u001b[39m _wait_until(\u001b[38;5;28;01mlambda\u001b[39;00m: _processing_job_status(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client, job), poll)\n\u001b[0;32m-> 4413\u001b[0m \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:6950\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   6945\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   6946\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6947\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6948\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6949\u001b[0m     )\n\u001b[0;32m-> 6950\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6951\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6952\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6953\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6954\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Processing job sm-spark-project-2023-10-29-17-54-53-000: Failed. Reason: OutOfMemoryError: Please use an instance type with more memory, or ensure that your processing container does not use more memory than available."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=6,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path_commments = \"s3://bigdatateaching/reddit-parquet/comments/year=****/month=*/*.parquet\"\n",
    "s3_dataset_path_submissions = \"s3://bigdatateaching/reddit-parquet/submissions/year=****/month=*/*.parquet\"\n",
    "output_prefix_data = \"project\"\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "subreddits = \"collegebasketball\"\n",
    "    \n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/process.py\",\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path_commments\",\n",
    "        s3_dataset_path_commments,\n",
    "        \"--s3_dataset_path_submissions\",\n",
    "        s3_dataset_path_submissions,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_prefix\",\n",
    "        output_prefix_data,\n",
    "        \"--subreddits\",\n",
    "        subreddits,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea72c8-a9c7-4655-95e6-0ca6c35aa238",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data}/comments\"\n",
    "print(f\"reading submissions from {s3_path}\")\n",
    "comments = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f745b53-aea2-4780-8893-df47c41aaf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data}/submissions\"\n",
    "print(f\"reading submissions from {s3_path}\")\n",
    "submissions = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc728f7-cc3c-4555-8946-512cecf337c2",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf80840-e6ee-4b08-bc45-fdd5878a27ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
