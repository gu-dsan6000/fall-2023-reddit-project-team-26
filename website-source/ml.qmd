# ML

Please find the code for this analysis in the notebooks located [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-26/tree/main/code/ml)

## Executive Summary

In our machine learning endeavors, we delved into various factors potentially influencing review popularity and employed a random forest model for predictive analysis. The findings reveal that the length of a comment, the author’s activity, and immediate temporal factors—like hours and days—are strongly associated with the likelihood of a comment's popularity. However, the model's predictive performance is suboptimal, exhibiting a discernible bias. This limitation is intimately linked to the inherent distribution of the comments scores, suggesting that the model's accuracy is contingent upon the underlying data characteristics.

Then, we use two different machine learning models to predict which team each commenter supports using features from the comments dataset. We use k-means and SVM machine learning techniques to train and test two models, then compare their accuracy and recall to determine which model is more effective for this goal. Both of these models did not perform well. The lack of performance can be attributed to both the lack of predictive ability in the comments data and also the large number of potential teams a commenter can support. 

## Analysis Report

*A note: the business goals have changed from our original goals. Upon further exploration of the dataset, it has become apparent that our initial business objectives no longer align with the insights gleaned from the data. For instance, the anticipated correlation between the sentiment of comments and contest outcomes did not materialize as strongly as expected, casting doubt on the practicality of leveraging comment sentiment to forecast contest results. Consequently, we have pivoted our focus to previously unexamined aspects of the data, such as the popularity of comments, to ensure that our business goals remain rooted in the actionable intelligence derived from our analysis.*

### Business Goal 9

**Business Goal**: *Predict which comments are more popular than others in the College Basketball subreddit*

**Technical Proposal**: *Determine why certain comments are more popular than others in the College Basketball subreddit. Using Random Forest Regressor to identify if factors such as sentiment, time of post, date of post, comment length, activity of author, and whether or not the author has a team icon for their profile picture are relevant in determining how popular a post will be.*

Scores Distribution
In this section, popularity is determined by “score” which is calculated by upvotes-downvotes on the comment. 

::: {#fig-table1 layout-nrow=1}

![](../data/plots/ml_q1_table1.png){fig-align="center" width="500"}

The summary statistics for the "Score" variable in the comments dataframe. Score can be used to represent popularity.
:::

It is clear that there is a wide variation in the scores, with many posts deviating significantly from the average. To enhance readability, we focus on scores range from -20 to 80.

::: {#fig-plot1 layout-nrow=1}

![](../data/plots/ml_q1_score_dist.png){fig-align="center"}

The distribution of the "score" variable in the comments dataset. The x axis is limited to -20 to 80 so that we can focus on the majority of the data. Values go up to 5360
:::

According to the histogram, the scores are heavily concentrated around the lower end, indicating that the majority of posts receive a score within a narrow range close to zero. This is consistent with typical social media behavior, where most content receives little engagement. The distribution is right-skewed, meaning there are a few posts with very high scores, but these are exceptions rather than the rule. This skewness can affect predictive modeling because it suggests that high scores are outliers.

**Simple regression model**

We attempt to predict score of a comment (popularity) based on the following:
- team_index: whether the author has a supported team in their profile picture
- month_vec: what month the post is from
- sentiment_vec: what the sentiment of the post is
- year: what Year the post is from
- day: what day the post is from
- hour: what Hour of day the post is from
- comment_length: length of the post
- author_activity: how much the author has commented in this dataset

The first model is a simple random forest with `maxDepth=5`, and `numTrees=50`.

::: {#fig-plot2 layout-nrow=1}

![](../data/plots/ml_q1_simplemodel_plot.png){fig-align="center"}

True score vs predicted score for the simple random forest. X axis is limited to -20 to 80 so that the plot is more readable. Data has been downsampled from the full test set to show ~10,000 points, for readability.
:::

The Root Mean Squared Error (RMSE) stands at 37.55, a figure that significantly overshadows the expected range of scores, suggesting that the model's predictions often miss the mark by a wide margin. Additionally, an examination of the scatter plot reveals a pronounced accumulation of data points in the lower score region, with a conspicuous absence of predicted values in the upper echelons of actual scores. This pattern implies a propensity of the model to approximate towards the median, underscoring its limitations in forecasting scores at both ends of the spectrum. The model's inclination to underestimate particularly high scores is especially apparent. Such limitations could stem from the inherent simplicity of the model, the characteristics of the chosen features, or the predominant concentration of lower scores within the training data. Typically on Reddit, a platform characterized by its right-skewed score distribution, posts garnering a vast number of upvotes are exceptional. Consequently, the model's struggle to predict these rare, high-scoring outliers is not an anomaly but rather a challenge commonly encountered in the predictive modeling of social media interactions.

**Complex regression model**

Increasing the number of trees in the random forest from 50 to 200 to give the model more complexity. Also increasing the max depth from 5 to 10 to increase complexity as well.

::: {#fig-plot3 layout-nrow=1}

![](../data/plots/ml_q1_complexmodel_plot.png){fig-align="center"}

True score vs predicted score for the more complex random forest. X axis is limited to -20 to 80 so that the plot is more readable. Data has been downsampled from the full test set to show ~10,000 points, for readability.
:::

As the model's complexity is enhanced, there is a marginal reduction in the RMSE to 37.25, suggesting a nuanced improvement in predictive accuracy. The model now presents a broader spectrum of predicted scores, implying a more nuanced capture of the data's inherent variability and potentially a more accurate reflection of the underlying dynamics.

Notably, the central concentration of predicted scores persists, yet there is a discernible uptick in the model's proficiency at approximating higher scores, particularly those under 40. This improvement is likely attributable to the model's deepened analytical capacity, which now teases out more intricate data patterns. However, the model continues to show limitations in predicting very low scores, with no predictions falling below zero.

Despite the enhanced capacity to predict higher scores, a tangible disconnect remains between the uppermost actual scores and their predicted counterparts. The model demonstrates a persistent, albeit slightly reduced, propensity to favor a more constrained range of predicted scores than what the actual scores exhibit.
In summation, the more complex model displays an expanded prediction range and a modest enhancement in forecasting higher scores. Yet, it is imperative to weigh these incremental advancements against potential overfitting risks and the demand for greater computational resources. Consequently, increasing complexity brings no clear advantages.

Finally let’s check the model’s feature importance table.

::: {#fig-table2 layout-nrow=1}

![](../data/plots/ml_q1_table2.png){fig-align="center" width="300"}

Variable importance in the complex random forest model.
:::

The table indicates that comment length is the most critical determinant of popularity according to the model, with its high importance score implying that lengthier comments, possibly due to their depth or visibility, tend to engage users more effectively. Author activity is also a significant predictor, nearly matching the influence of comment length, which suggests that authors who comment frequently may benefit from increased visibility or an established presence within the community, thus boosting the appeal of their posts.
Temporal factors like the time of day and the date of the posting are influential as well, hinting at a correlation with peak activity periods, possibly linked to significant events such as key games that drive user engagement.
The importance scores of other features are relatively low. For example, the sentiment of comments has little impact on the model’s predictions. This is consistent with our previous observation that sentiment is not strongly correlated with contest results, leading to a change in our business goals.

### Business Goal 10
**Business Goal 10:** Build a model that predicts the results of Kansas games. \

**Technical Proposal**: Use sentiment analysis and comment attributes to build a machine learning model to predict the outcome of Kansas Jayhawks games. Use date columns and sentiment as feature inputs into this machine learning model. Use different machine learning models and evaluate performance based on confusion matrices. 

To answer this question, we were working to see if the result of a basketball game could be extracted from the time and the sentiment of the comments on the day before the game and the day after the game.

The available dataset included the comments under three subreddit including ‘CollegeBasketball’, ‘jayhawks’, and ‘tarheels’. We chose to focus on the Kansas games, so the comments in the “tarheels” subreddit are not relevant for this goal. Therefore, we filtered the dataset to get the comments in the both ‘CollegeBasketball’ and “jayhawks” subreddits and selected the columns including “day”, “month”, “year”, “week” and “col”. The target column is in the external dataset named “kansas_schedule.csv” in the folder “data/csv”. This external dataset is about the results of the games where Kansas played, and the results information is in the column named “Unnamed: 8”. Thus, we joined these two data frames together so that we could use machine learning pipelines to do prediction analysis. 

We tried three different models for classification: two random forest models with varying number of trees (model 1 using random forest with 20 trees and model 3 with 10 trees) and model 2 using logistic regression. Then we compared the results and got the table below.

::: {#fig-plot6 layout-nrow=1}

![](../data/plots/ml_q3_table.png){fig-align="center" width="400"}

Table of three machine learning models and their accuracies.
:::

From this table, we chose the third model, a random forest model with ten trees to make further analysis by the confusion matrix. And then, we got the confusion matrix like the graph below.

::: {#fig-plot7 layout-nrow=1}

![](../data/plots/ml_q3_cm.png){fig-align="center" width="400"}

Confusion Matrix of random forest machine learning model.
:::

From the confusion matrix, we were surprised that the prediction performance was satisfactory. It seems that using the time and the sentiment of the comments can help make predictions for the result of the upcoming Kansas game, which is interesting.


### Conclusion

In this analysis we answered two business questions, one trying to predict popularity of comments, and the other trying to predict which team an author supports. In both cases we found that the available data in the reddit posts and information about the posts were not very predictive of popularity or who the author supports. Moving forward we could attempt more complex models, but with the lack of success in these models we don't think that would lead to much better results.


### Business Goal 11

**Business Goal:** *Predict which team each commenter supports using the comments themselves.*

**Technical Proposal**: *Using two different machine learning models to predict which team each commenter supports using features from the comments dataset. Use kmeans and SVM machine learning techniques to train and test two models, then compare their accuracy and recall to determine which model is more effective for this goal. Complete other visualizations using these techniques to further examine the dataset.*

For the second part of this section, we chose to to use two models: one kmeans model and one SVM model. We intended to use each comment to predict which team each commenter supported. Our accuracy scores were very low for this portion, which we will discuss below.

For the first kmeans model, we got the following results:

::: {#fig-table3 layout-nrow=1}

![](../data/plots/ml_q2_table1.png){fig-align="center" width="400"}

Results for the kmeans model
:::

This model alone shows us a few things. First, it has a very low accuracy. That tells us that it is very difficult to predict which team each commenter supports just by using what they comment on. That leads us to our second conclusion: that posters write about things beyond their own teams. This gives us insight into the college basketball poster atmosphere writ large, as people are interested in teams outside of just their own bubble. Now, we'll see if we can increase the accuracy of prediction by using a different model.

For the second model, the SVM model:

::: {#fig-table4 layout-nrow=1}

![](../data/plots/ml_q2_table2.png){fig-align="center" width="400"}

Results for the svm model
:::

Interesting! This model had a very slightly higher accuracy, although not by much. This confirms our analysis from running the first model: that predictions based on what people comment about are very hard, and one theory behind that is that college basketball has a robust community of commenters who each care about games beyond who their own team is. This is also relevant because at the end of the season, 64 teams are in a tournament to win the championship, so each fan needs to be very engaged to be ready for the end of the season. This is one theory about why these accuracy scores shown here are so low; there are certainly others.

Additionally, here is a chart (a confusion matrix) to show how the SVM model performed:

::: {#fig-plot4 layout-nrow=1}

![](../data/plots/ml_q2_cm.png){fig-align="center" width="400"}

Confusion matrix for the SVM model
:::

As you can see, the confusion matrix revealed that most, if not all, of the model was extremely inaccurate. There were slight trends in a few areas that the model has some success predicting, but overall, this was not an accurate result, and this model does not have any predictive power.

As an additional section, we conducted some additional machine learning on using kmeans to create word clouds out of three clusters that we initially created. In these word clouds, the most common words in each cluster were:

- **Cluster 0 Top Keywords:** just, lol, team, don, year, think, fuck, time, big, foul
- **Cluster 1 Top Keywords:** like, good, just, team, looks, don, really, year, feel, think
- **Cluster 2 Top Keywords:** game, just, like, win, good, team, refs, play, ve, watch


::: {#fig-plot5 layout-nrow=3}

![](../data/plots/wordcloud0.png){fig-align="center" width="400"}
![](../data/plots/wordcloud1.png){fig-align="center" width="400"}
![](../data/plots/wordcloud2.png){fig-align="center" width="400"}

Word Cloud for the K-means model clusters
:::

### Business Goal 10
**Business Goal 10:** Build a model that predicts the results of Kansas games. \
**Technical Proposal**: Use sentiment analysis and comment attributes to build a machine learning model to predict the outcome of Kansas Jayhawks games. Use date columns and sentiment as feature inputs into this machine learning model. Use different machine learning models and evaluate performance based on confusion matrices. 

To answer this question, we were working to see if the result of a basketball game could be extracted from the time and the sentiment of the comments on the day before the game and the day after the game.

The available dataset included the comments under three subreddit including ‘CollegeBasketball’, ‘jayhawks’, and ‘tarheels’. We chose to focus on the Kansas games, so the comments in the “tarheels” subreddit are not relevant for this goal. Therefore, we filtered the dataset to get the comments in the both ‘CollegeBasketball’ and “jayhawks” subreddits and selected the columns including “day”, “month”, “year”, “week” and “col”. The target column is in the external dataset named “kansas_schedule.csv” in the folder “data/csv”. This external dataset is about the results of the games where Kansas played, and the results information is in the column named “Unnamed: 8”. Thus, we joined these two data frames together so that we could use machine learning pipelines to do prediction analysis. 

We tried three different models for classification: two random forest models with varying number of trees (model 1 using random forest with 20 trees and model 3 with 10 trees) and model 2 using logistic regression. Then we compared the results and got the table below.

::: {#fig-plot6 layout-nrow=1}

![](../data/plots/ml_q3_table.png){fig-align="center" width="400"}

Table of three machine learning models and their accuracies.
:::

From this table, we chose the third model, a random forest model with ten trees to make further analysis by the confusion matrix. And then, we got the confusion matrix like the graph below.

::: {#fig-plot7 layout-nrow=1}

![](../data/plots/ml_q3_cm.png){fig-align="center" width="400"}

Confusion Matrix of random forest machine learning model.
:::

From the confusion matrix, we were surprised that the prediction performance was satisfactory. It seems that using the time and the sentiment of the comments can help make predictions for the result of the upcoming Kansas game, which is interesting.


### Conclusion

In this analysis we answered two business questions, one trying to predict popularity of comments, and the other trying to predict which team an author supports. In both cases we found that the available data in the reddit posts and information about the posts were not very predictive of popularity or who the author supports. Moving forward we could attempt more complex models, but with the lack of success in these models we don't think that would lead to much better results.